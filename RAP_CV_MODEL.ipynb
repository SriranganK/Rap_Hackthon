{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Esy5qbqzeQr",
        "outputId": "316b3acd-5fa8-44f6-ecf9-8a4f6b5670a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shwVrB8hr0fv",
        "outputId": "ebfd039b-9737-4e25-c616-6eb5dafcb0ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 3s 0us/step\n",
            "Found 165 images belonging to 3 classes.\n",
            "Found 49 images belonging to 3 classes.\n",
            "Found 50 images belonging to 3 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/PIL/Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "6/6 [==============================] - 102s 16s/step - loss: 0.7090 - accuracy: 0.6909 - val_loss: 0.4292 - val_accuracy: 0.8571\n",
            "Epoch 2/15\n",
            "6/6 [==============================] - 29s 5s/step - loss: 0.3603 - accuracy: 0.9030 - val_loss: 0.4045 - val_accuracy: 0.8776\n",
            "Epoch 3/15\n",
            "6/6 [==============================] - 30s 5s/step - loss: 0.3256 - accuracy: 0.8970 - val_loss: 0.4084 - val_accuracy: 0.8367\n",
            "Epoch 4/15\n",
            "6/6 [==============================] - 30s 5s/step - loss: 0.3445 - accuracy: 0.8848 - val_loss: 0.3251 - val_accuracy: 0.8980\n",
            "Epoch 5/15\n",
            "6/6 [==============================] - 30s 5s/step - loss: 0.2482 - accuracy: 0.9091 - val_loss: 0.3126 - val_accuracy: 0.8980\n",
            "Epoch 6/15\n",
            "6/6 [==============================] - 30s 5s/step - loss: 0.2455 - accuracy: 0.9152 - val_loss: 0.2310 - val_accuracy: 0.8776\n",
            "Epoch 7/15\n",
            "6/6 [==============================] - 29s 5s/step - loss: 0.2599 - accuracy: 0.8970 - val_loss: 0.2969 - val_accuracy: 0.8980\n",
            "Epoch 8/15\n",
            "6/6 [==============================] - 33s 6s/step - loss: 0.2267 - accuracy: 0.9212 - val_loss: 0.2085 - val_accuracy: 0.8980\n",
            "Epoch 9/15\n",
            "6/6 [==============================] - 29s 5s/step - loss: 0.2116 - accuracy: 0.9152 - val_loss: 0.2365 - val_accuracy: 0.8980\n",
            "Epoch 10/15\n",
            "6/6 [==============================] - 30s 5s/step - loss: 0.1906 - accuracy: 0.9273 - val_loss: 0.1820 - val_accuracy: 0.9184\n",
            "Epoch 11/15\n",
            "6/6 [==============================] - 29s 6s/step - loss: 0.3603 - accuracy: 0.8485 - val_loss: 0.1764 - val_accuracy: 0.9184\n",
            "Epoch 12/15\n",
            "6/6 [==============================] - 41s 7s/step - loss: 0.2489 - accuracy: 0.9212 - val_loss: 0.2749 - val_accuracy: 0.8776\n",
            "Epoch 13/15\n",
            "6/6 [==============================] - 29s 5s/step - loss: 0.2899 - accuracy: 0.9030 - val_loss: 0.2489 - val_accuracy: 0.8571\n",
            "Epoch 14/15\n",
            "6/6 [==============================] - 30s 5s/step - loss: 0.1776 - accuracy: 0.9455 - val_loss: 0.1300 - val_accuracy: 0.9388\n",
            "Epoch 15/15\n",
            "6/6 [==============================] - 32s 5s/step - loss: 0.1899 - accuracy: 0.9333 - val_loss: 0.1341 - val_accuracy: 0.9184\n",
            "2/2 [==============================] - 14s 12s/step - loss: 0.2479 - accuracy: 0.9000\n",
            "Test accuracy: 89.99999761581421\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "img_shape = (700,1000, 3)\n",
        "\n",
        "\n",
        "\n",
        "inception = InceptionV3(weights='imagenet', include_top=False, input_shape=img_shape)\n",
        "for layer in inception.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "x = GlobalAveragePooling2D()(inception.output)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dense(3, activation='softmax')(x)\n",
        "\n",
        "\n",
        "model = Model(inputs=inception.input, outputs=x)\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory('/content/drive/MyDrive/CV_Dataset/train', target_size=img_shape[:2], batch_size=32, class_mode='categorical')\n",
        "val_generator = val_datagen.flow_from_directory('/content/drive/MyDrive/CV_Dataset/val', target_size=img_shape[:2], batch_size=32, class_mode='categorical')\n",
        "test_generator = test_datagen.flow_from_directory('/content/drive/MyDrive/CV_Dataset/test', target_size=img_shape[:2], batch_size=32, class_mode='categorical')\n",
        "\n",
        "\n",
        "history = model.fit(train_generator, epochs=15, validation_data=val_generator)\n",
        "\n",
        "\n",
        "loss, acc = model.evaluate(test_generator)\n",
        "print('Test accuracy:', acc*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/document_class15-epoch.model',save_format='h5')"
      ],
      "metadata": {
        "id": "zsE35HiH9-Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "img_path = '/content/drive/MyDrive/CV_Dataset/scraped-val/scientific_publication/item1313077_600px.jpeg'\n",
        "img = image.load_img(img_path, target_size=(700, 1000))\n",
        "\n",
        "\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "l_model = tf.keras.models.load_model('/content/drive/MyDrive/document_class15-epoch.model')\n",
        "\n",
        "predictions = l_model.predict(x)\n",
        "result_class = np.argmax(predictions, axis = -1)\n",
        "\n",
        "print(result_class , predictions)\n",
        "\n",
        "print(predictions[0][result_class])\n",
        "\n",
        "if result_class == 0:\n",
        "  print('Email')\n",
        "elif result_class == 1:\n",
        "  print('Resume')\n",
        "else :\n",
        "  print('Scientic Publication')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-wrrQ4d7d6O",
        "outputId": "2799b0b3-d927-4d73-fd2f-a28648ca535e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "[2] [[2.7503687e-04 1.9891018e-03 9.9773586e-01]]\n",
            "[0.99773586]\n",
            "Scientic Computing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validating using web scraped dataset"
      ],
      "metadata": {
        "id": "FNCIKYGMLzCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "img_shape = (700,1000, 3)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "sval_generator = val_datagen.flow_from_directory('/content/drive/MyDrive/CV_Dataset/scraped-val', target_size=img_shape[:2], batch_size=32, class_mode='categorical')\n",
        "val_loss, val_acc =l_model.evaluate(sval_generator)\n",
        "print('Validation accuracy:', val_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VX2WuLngS8z",
        "outputId": "feb04bdb-2e14-4221-b19d-7e3855be1b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 74 images belonging to 3 classes.\n",
            "3/3 [==============================] - 11s 4s/step - loss: 1.0221 - accuracy: 0.5946\n",
            "Validation accuracy: 0.5945945978164673\n"
          ]
        }
      ]
    }
  ]
}